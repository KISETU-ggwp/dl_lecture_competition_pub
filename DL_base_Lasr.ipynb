{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOCNlKsoJ1A7AejrH2CRtMY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 前準備"
      ],
      "metadata": {
        "id": "VjotR0bv6i5Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1YgkjEbYwUyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18449b8c-1f56-4eb0-9525-2368c1379dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dl_lecture_competition_pub'...\n",
            "Host key verification failed.\r\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ]
        }
      ],
      "source": [
        "! git clone git@github.com:KISETU-ggwp/dl_lecture_competition_pub.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "_AdIpd802f32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ba35dc68-4435-4096-808f-8c1158e72555"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "from statistics import mode\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "d2l2nKgEy9vs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ2Uhpdd3WzA",
        "outputId": "dba664fc-54a4-4ca8-b698-80f42f52bcda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visual Question Answering（VQA）"
      ],
      "metadata": {
        "id": "27tfhaw_6cWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# 様々な形式のテキストを一貫した形に整理\n",
        "def process_text(text):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 数詞を数字に変換\n",
        "    num_word_to_digit = {\n",
        "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
        "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
        "        'ten': '10'\n",
        "    }\n",
        "    for word, digit in num_word_to_digit.items():\n",
        "        text = text.replace(word, digit)\n",
        "\n",
        "    # 小数点のピリオドを削除\n",
        "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
        "\n",
        "    # 冠詞の削除\n",
        "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
        "\n",
        "    # 短縮形のカンマの追加\n",
        "    contractions = {\n",
        "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
        "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
        "    }\n",
        "    for contraction, correct in contractions.items():\n",
        "        text = text.replace(contraction, correct)\n",
        "\n",
        "    # 句読点をスペースに変換\n",
        "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
        "\n",
        "    # 句読点をスペースに変換\n",
        "    text = re.sub(r'\\s+,', ',', text)\n",
        "\n",
        "    # 連続するスペースを1つに変換\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "i4rWru6y1exj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. データローダーの作成"
      ],
      "metadata": {
        "id": "3xFztOcT6CuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df_path, image_dir, transform=None, answer=True):\n",
        "        self.transform = transform  # 画像の前処理\n",
        "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
        "        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
        "        self.answer = answer\n",
        "\n",
        "        # question / answerの辞書を作成\n",
        "        self.question2idx = {}\n",
        "        self.answer2idx = {}\n",
        "        self.idx2question = {}\n",
        "        self.idx2answer = {}\n",
        "\n",
        "        # 質問文に含まれる単語を辞書に追加\n",
        "        for question in self.df[\"question\"]:\n",
        "            question = process_text(question)\n",
        "            words = question.split(\" \")\n",
        "            for word in words:\n",
        "                if word not in self.question2idx:\n",
        "                    self.question2idx[word] = len(self.question2idx)\n",
        "        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n",
        "\n",
        "        if self.answer:\n",
        "            # 回答に含まれる単語を辞書に追加\n",
        "            for answers in self.df[\"answers\"]:\n",
        "                for answer in answers:\n",
        "                    word = answer[\"answer\"]\n",
        "                    word = process_text(word)\n",
        "                    if word not in self.answer2idx:\n",
        "                        self.answer2idx[word] = len(self.answer2idx)\n",
        "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
        "\n",
        "    def update_dict(self, dataset):\n",
        "        \"\"\"\n",
        "        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : Dataset\n",
        "            訓練データのDataset\n",
        "        \"\"\"\n",
        "        self.question2idx = dataset.question2idx\n",
        "        self.answer2idx = dataset.answer2idx\n",
        "        self.idx2question = dataset.idx2question\n",
        "        self.idx2answer = dataset.idx2answer\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        対応するidxのデータ（画像，質問，回答）を取得．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            取得するデータのインデックス\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        image : torch.Tensor  (C, H, W)\n",
        "            画像データ\n",
        "        question : torch.Tensor  (vocab_size)\n",
        "            質問文をone-hot表現に変換したもの\n",
        "        answers : torch.Tensor  (n_answer)\n",
        "            10人の回答者の回答のid\n",
        "        mode_answer_idx : torch.Tensor  (1)\n",
        "            10人の回答者の回答の中で最頻値の回答のid\n",
        "        \"\"\"\n",
        "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
        "        image = self.transform(image)\n",
        "        question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\n",
        "        question_words = self.df[\"question\"][idx].split(\" \")\n",
        "        for word in question_words:\n",
        "            try:\n",
        "                question[self.question2idx[word]] = 1  # one-hot表現に変換\n",
        "            except KeyError:\n",
        "                question[-1] = 1  # 未知語\n",
        "\n",
        "        if self.answer:\n",
        "            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
        "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
        "\n",
        "            return image, torch.Tensor(question), torch.Tensor(answers), int(mode_answer_idx)\n",
        "\n",
        "        else:\n",
        "            return image, torch.Tensor(question)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n"
      ],
      "metadata": {
        "id": "3FPdmsKw1O3H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 評価指標の実装"
      ],
      "metadata": {
        "id": "pflx6WKk6G4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 簡単にするならBCEを利用する\n",
        "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
        "    total_acc = 0.\n",
        "\n",
        "    for pred, answers in zip(batch_pred, batch_answers):\n",
        "        acc = 0.\n",
        "        for i in range(len(answers)):\n",
        "            num_match = 0\n",
        "            for j in range(len(answers)):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if pred == answers[j]:\n",
        "                    num_match += 1\n",
        "            acc += min(num_match / 3, 1)\n",
        "        total_acc += acc / 10\n",
        "\n",
        "    return total_acc / len(batch_pred)"
      ],
      "metadata": {
        "id": "IMV1p7ZB1TCP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. モデルのの実装"
      ],
      "metadata": {
        "id": "_qQeTzJ06KuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNetを利用できるようにしておく\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "QUAG_Wce1Vtn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleneckBlock(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "7PuGI4ZI6SlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, layers[0], 64)\n",
        "        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\n",
        "        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\n",
        "        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, 512)\n",
        "\n",
        "    def _make_layer(self, block, blocks, out_channels, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(BottleneckBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, n_answer: int):\n",
        "        super().__init__()\n",
        "        self.resnet = ResNet18()\n",
        "        self.text_encoder = nn.Linear(vocab_size, 512)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, n_answer)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, question):\n",
        "        image_feature = self.resnet(image)  # 画像の特徴量\n",
        "        question_feature = self.text_encoder(question)  # テキストの特徴量\n",
        "\n",
        "        x = torch.cat([image_feature, question_feature], dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "N_oB1CbF6Uxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 学習の実装"
      ],
      "metadata": {
        "id": "5kouPkIK6OkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    simple_acc = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for image, question, answers, mode_answer in dataloader:\n",
        "        image, question, answer, mode_answer = \\\n",
        "            image.to(device), question.to(device), answers.to(device), mode_answer.to(device)\n",
        "\n",
        "        pred = model(image, question)\n",
        "        loss = criterion(pred, mode_answer.squeeze())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
        "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
        "\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
        "\n"
      ],
      "metadata": {
        "id": "GgY3NsA91YqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, dataloader, optimizer, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    simple_acc = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for image, question, answers, mode_answer in dataloader:\n",
        "        image, question, answer, mode_answer = \\\n",
        "            image.to(device), question.to(device), answers.to(device), mode_answer.to(device)\n",
        "\n",
        "        pred = model(image, question)\n",
        "        loss = criterion(pred, mode_answer.squeeze())\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
        "        simple_acc += (pred.argmax(1) == mode_answer).mean().item()  # simple accuracy\n",
        "\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
        "\n"
      ],
      "metadata": {
        "id": "6IDJqiih7kKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # deviceの設定\n",
        "    set_seed(42)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # dataloader / model\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    train_dataset = VQADataset(df_path=\"/content/drive/MyDrive/DL_base/DL_base_Last/data/train.json\", image_dir=\"/content/drive/MyDrive/DL_base/DL_base_Last/data/train\", transform=transform)\n",
        "    test_dataset = VQADataset(df_path=\"/content/drive/MyDrive/DL_base/DL_base_Last/data/valid.json\", image_dir=\"/content/drive/MyDrive/DL_base/DL_base_Last/data/valid\", transform=transform, answer=False)\n",
        "    test_dataset.update_dict(train_dataset)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    model = VQAModel(vocab_size=len(train_dataset.question2idx)+1, n_answer=len(train_dataset.answer2idx)).to(device)\n",
        "\n",
        "    # optimizer / criterion\n",
        "    num_epoch = 20\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "    # train model\n",
        "    for epoch in range(num_epoch):\n",
        "        train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
        "              f\"train time: {train_time:.2f} [s]\\n\"\n",
        "              f\"train loss: {train_loss:.4f}\\n\"\n",
        "              f\"train acc: {train_acc:.4f}\\n\"\n",
        "              f\"train simple acc: {train_simple_acc:.4f}\")\n",
        "\n",
        "    # 提出用ファイルの作成\n",
        "    model.eval()\n",
        "    submission = []\n",
        "    for image, question in test_loader:\n",
        "        image, question = image.to(device), question.to(device)\n",
        "        pred = model(image, question)\n",
        "        pred = pred.argmax(1).cpu().item()\n",
        "        submission.append(pred)\n",
        "\n",
        "    submission = [train_dataset.idx2answer[id] for id in submission]\n",
        "    submission = np.array(submission)\n",
        "    torch.save(model.state_dict(), \"model.pth\")\n",
        "    np.save(\"/content/drive/MyDrive/DL_base/DL_base_Last/submission.npy\", submission)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "yPJSK6qh7muJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}